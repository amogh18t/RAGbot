{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install all required libraries**\n",
        "\n",
        "1. Huggingface libraries to use Mistral 7B LLM (Open Source LLM)\n",
        "\n",
        "2. LangChain library to call LLM to generate reposne based on the prompt"
      ],
      "metadata": {
        "id": "1aTJ_6UldPjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHQaMxXHJlBD"
      },
      "outputs": [],
      "source": [
        "# Huggingface libraries to run LLM.\n",
        "# We will understand in depth later\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required Libraries and check whether we have access to GPU or not.\n",
        "\n",
        "We must be getting following output after running the cell\n",
        "\n",
        "Device: cuda\n",
        "\n",
        "Tesla T4"
      ],
      "metadata": {
        "id": "Gt43HfdAdvjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from typing import List\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "tYB9unhcKZnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **We are using Mistral 7 Billion parameter LLM (Open source from HuggingFace)**\n",
        "\n",
        "If you're new to Hugging Face or machine learning tasks, the following lines of code introduce some scary concepts. This Mistral 7B LLM, although big in size (approximately 14 GB), presents a challenge when loading on a Colab notebook, requiring larger GPUs and making it incompatible with the free T4 GPU.\n",
        "\n",
        "To address this issue, we employ a sharded version of the Mistral LLM. This version is loaded in smaller increments (approximately 1.9 GB each) in the notebook and subsequently aggregated into a single file. This allows us to seamlessly utilize the LLM on a free Colab GPU (T4 GPU) without incurring any costs for learning purposes.\n",
        "\n",
        "These lines of code incorporate several important concepts, which may initially seem complex but are easily comprehensible:\n",
        "\n",
        "1. BitsAndBytesConfig:\n",
        "\n",
        "  - load_in_4bit\n",
        "  - bnb_4bit_use_double_quant\n",
        "  - bnb_4bit_quant_type\n",
        "  - bnb_4bit_compute_dtype\n",
        "\n",
        "2. AutoModelForCausalLM.from_pretrained\n",
        "\n",
        "3. AutoTokenizer.from_pretrained\n",
        "\n",
        "Despite the use of some technical terminology, don't be discouraged. In the coming days, we'll delve into these concepts in an approachable manner, breaking down the complexities. This includes a detailed understanding of the mentioned configurations, loading procedures, and tokenization processes. Embrace the learning journey; it's simpler than it seems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NR-fee8reJXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "origin_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig \\\n",
        "              (\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              )\n",
        "model = AutoModelForCausalLM.from_pretrained (model_path, trust_remote_code=True,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(origin_model_path)"
      ],
      "metadata": {
        "id": "pB86JDv3JOU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating pipelines to run LLM at Colab notebook**\n",
        "\n",
        "It uses\n",
        "\n",
        "1. Huggingface transformers pipeline\n",
        "  - This is an object that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
        "\n",
        "  - We are using \"text-generation\" task. It's sort of chatGPT where you ask question and model respond with answer of that question based on it's intelligence.\n",
        "\n",
        "\n",
        "2. LangChain HuggingFacePipeline\n",
        "  - Integrating \"transformers.pipeline\" with LangChain.\n",
        "  - You may be thinking why we need this. We will be using LangChain extensively later and this is first step to integrate our LLM with LangChain\n"
      ],
      "metadata": {
        "id": "jJkALiFLiLxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=100,\n",
        "    temperature = 0.5,\n",
        "    do_sample=True,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "MpPNufTqJ48O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ta-da! Brace yourself for the grand premiere of our Language Model (LLM)!**\n",
        "\n",
        "Fire away with your question prompt, and get ready to be spellbound by the magic that follows!"
      ],
      "metadata": {
        "id": "PGffkOIljoRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the purpose of our life in 100 words?\"\n",
        "response = mistral_llm.invoke(text)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "aoqBV5wiQQ8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}